# ğŸ“Š Projeto ETL â€“ Pipeline de Dados (Bronze â†’ Silver â†’ PostgreSQL)

Este projeto implementa um **pipeline ETL completo**, desde a ingestÃ£o de dados brutos atÃ© a persistÃªncia em banco de dados PostgreSQL, seguindo boas prÃ¡ticas de organizaÃ§Ã£o em camadas (**Bronze / Silver**) e preparaÃ§Ã£o para anÃ¡lise exploratÃ³ria.

O objetivo Ã© demonstrar habilidades em **engenharia de dados**, **tratamento de dados estruturados e semi-estruturados**, **persistÃªncia em banco relacional** e **anÃ¡lise exploratÃ³ria com Python**.

---

## ğŸ—ï¸ Arquitetura do Projeto

Projeto_ETL/
â”œâ”€â”€ 01.bronze.raw/
â”‚   â”œâ”€â”€ users.csv
â”‚   â””â”€â”€ cryptocurrencies.json
â”‚
â”œâ”€â”€ 02.silver.validated/
â”‚
â”œâ”€â”€ normalize_data.py
â”œâ”€â”€ load_to_db.py
â”œâ”€â”€ db.py
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Data_View_ETL.ipynb
â””â”€â”€ README.md


## ğŸ”„ Fluxo do Pipeline ETL

### 1ï¸âƒ£ Camada Bronze â€“ Dados Brutos
- Dados armazenados sem tratamento
- Formatos suportados:
  - CSV
  - JSON (padrÃ£o e line-delimited)
- Nenhuma modificaÃ§Ã£o estrutural Ã© feita nesta camada

---

### 2ï¸âƒ£ Camada Silver â€“ Dados Tratados
O script `normalize_data.py` executa as seguintes etapas:

- Leitura dos arquivos da camada Bronze
- ConversÃ£o de colunas do tipo `list` para `string`
- RemoÃ§Ã£o de registros duplicados
- PadronizaÃ§Ã£o do formato de saÃ­da
- PersistÃªncia dos dados em **Parquet**

Essa etapa garante dados **consistentes, deduplicados e prontos para consumo analÃ­tico**.

---

### 3ï¸âƒ£ PersistÃªncia em Banco de Dados (PostgreSQL)
Os arquivos Parquet da camada Silver sÃ£o carregados no PostgreSQL utilizando:

- Docker + docker-compose
- Classe `Database` para:
  - Criar tabelas dinamicamente a partir dos DataFrames
  - Inserir dados no banco
  - Executar consultas SQL

Cada arquivo Parquet gera automaticamente uma tabela no banco.

---

## ğŸ³ Banco de Dados (Docker)

O PostgreSQL Ã© executado via Docker com a seguinte configuraÃ§Ã£o:

- **UsuÃ¡rio:** postgres  
- **Senha:** postgres  
- **Banco:** postgres  
- **Porta:** 5432  

O volume Ã© persistente, garantindo que os dados nÃ£o sejam perdidos ao reiniciar o container.

Para subir o banco de dados:

```bash
docker-compose up -d

ğŸ“ˆ AnÃ¡lise ExploratÃ³ria de Dados

O notebook Data_View_ETL.ipynb realiza:

VisualizaÃ§Ã£o inicial dos dados de usuÃ¡rios

EstatÃ­sticas descritivas

CÃ¡lculo de idade a partir da data de nascimento

DistribuiÃ§Ã£o de idades e gÃªnero

AnÃ¡lise de criptomoedas por market cap

GrÃ¡ficos simples para validaÃ§Ã£o dos dados

Essa etapa valida a qualidade dos dados apÃ³s o ETL e demonstra preparo para anÃ¡lises posteriores.

ğŸ§° Tecnologias Utilizadas

Python

Pandas

PostgreSQL

Docker / Docker Compose

Parquet

Matplotlib

Jupyter Notebook

â–¶ï¸ Como Executar o Projeto

1ï¸âƒ£ Subir o banco de dados:

docker-compose up -d


2ï¸âƒ£ Executar o pipeline de normalizaÃ§Ã£o:

python normalize_data.py


3ï¸âƒ£ Carregar os dados no PostgreSQL:

python load_to_db.py


4ï¸âƒ£ Abrir o notebook para anÃ¡lise exploratÃ³ria:

jupyter notebook Data_View_ETL.ipynb

ğŸ¯ Objetivo do Projeto

Este projeto foi desenvolvido com foco em portfÃ³lio, demonstrando:

OrganizaÃ§Ã£o de dados em camadas (Lakehouse)

Tratamento de dados semi-estruturados

CriaÃ§Ã£o de pipelines ETL reutilizÃ¡veis

PersistÃªncia em banco relacional

ValidaÃ§Ã£o e anÃ¡lise exploratÃ³ria dos dados
